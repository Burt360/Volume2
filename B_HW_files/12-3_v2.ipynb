{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccc2d7f0",
   "metadata": {},
   "source": [
    "# Section 12.3, Exercises 12.15-12.17\n",
    "\n",
    "Nathan Schill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2335378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "Df = optimize.approx_fprime\n",
    "newton = optimize.newton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d056c8",
   "metadata": {},
   "source": [
    "## 12.15 - Exact Gradient Descent for Quadratic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56cf6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.99999953,  0.50000031])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quad_grad_descent(Q, b, x0, eps=1e-6):\n",
    "    # Max number of iterations in case of failure to converge\n",
    "    max_iter = 1000\n",
    "\n",
    "    # Compute xk using the exact gradient descent algorithm\n",
    "    xk = x0\n",
    "    for _ in range(max_iter):\n",
    "        # Get Df(xk)^T\n",
    "        DfxkT = Q@xk - b\n",
    "\n",
    "        # Return xk if ||DfxkT|| < eps\n",
    "        if np.linalg.norm(DfxkT) < eps:\n",
    "            return xk\n",
    "\n",
    "        # Compute next iteration\n",
    "        alpha = (DfxkT.T@DfxkT) / (DfxkT.T@Q@DfxkT)\n",
    "        xk = xk - alpha * DfxkT\n",
    "\n",
    "    return None      \n",
    "\n",
    "Q = np.array([[2, -2],\n",
    "              [-2, 4]])\n",
    "b = np.array([-3, 4])\n",
    "x0 = np.array([2,0])\n",
    "\n",
    "quad_grad_descent(Q, b, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d38ad",
   "metadata": {},
   "source": [
    "## 12.16 - Exact Gradient Descent (general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce76c7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.99999898,  0.50000063])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def line_search(f, xk, DfxkT):\n",
    "    '''Return a critical point alpha which we hope is a minimizer.'''\n",
    "    # Initial alpha guess: Use this because Example 12.3.2\n",
    "    # found alpha=0.1 to be a minimizer for its problem\n",
    "    a0 = 0.1\n",
    "    \n",
    "    # Want to minimize f(xk - a*DfxkT) wrt a\n",
    "    g = lambda a: f(xk - a*DfxkT)\n",
    "\n",
    "    # Create function that approximates derivative of g at the input\n",
    "    tol = np.sqrt(np.finfo(float).eps)\n",
    "    Dg = lambda x: Df(x, g, tol)\n",
    "\n",
    "    # Return the minimizer alpha\n",
    "    return newton(Dg, a0, maxiter=100, disp=False)\n",
    "\n",
    "\n",
    "def exact_grad_descent(f, x0, eps=1e-6):\n",
    "    # Max number of iterations in case of failure to converge\n",
    "    max_iter = 10000\n",
    "\n",
    "    # tol to use for finite difference quotient approximation of Df(xk)\n",
    "    tol = np.sqrt(np.finfo(float).eps)\n",
    "\n",
    "    # Compute xk using the exact gradient descent algorithm\n",
    "    xk = x0\n",
    "    for _ in range(max_iter):\n",
    "        # Get Df(xk)^T\n",
    "        DfxkT = Df(xk, f, tol).T\n",
    "\n",
    "        # Return xk if ||DfxkT|| < eps\n",
    "        if np.linalg.norm(DfxkT) < eps:\n",
    "            return xk\n",
    "\n",
    "        # Compute next iteration\n",
    "        alpha = line_search(f, xk, DfxkT)\n",
    "        xk = xk - alpha * DfxkT\n",
    "        \n",
    "    return 'Did not converge'\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    Q = np.array([[2, -2],\n",
    "              [-2, 4]])\n",
    "    b = np.array([-3, 4])\n",
    "    return 1/2 * x@Q@x - b@x + 5\n",
    "\n",
    "x0 = np.array([2,0])\n",
    "f(x0)\n",
    "\n",
    "exact_grad_descent(f, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e563f19",
   "metadata": {},
   "source": [
    "## 12.17 - Exact G.D. on Rosenbrock function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93c57229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num iters: 8230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.9999999 , 1.00000015])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify function for this problem to return when estimate is within 1e-5 of the true minimizer (1,1)\n",
    "# since the function from the previous problem checks the norm of the derivative...\n",
    "def exact_grad_descent(f, x0, eps=1e-6):\n",
    "    true_minimizer = np.array([1,1])\n",
    "\n",
    "    # Max number of iterations in case of failure to converge\n",
    "    max_iter = 10000\n",
    "\n",
    "    # tol to use for finite difference quotient approximation of Df(xk)\n",
    "    tol = np.sqrt(np.finfo(float).eps)\n",
    "\n",
    "    # Compute xk using the exact gradient descent algorithm\n",
    "    xk = x0\n",
    "    for i in range(max_iter):\n",
    "        # Get Df(xk)^T\n",
    "        DfxkT = Df(xk, f, tol).T\n",
    "\n",
    "        # Return xk if ||DfxkT|| < eps\n",
    "        if np.linalg.norm(xk - true_minimizer) < 1e-5 or np.linalg.norm(DfxkT) < eps:\n",
    "            print('Num iters:', i)\n",
    "            return xk\n",
    "\n",
    "        # Compute next iteration\n",
    "        alpha = line_search(f, xk, DfxkT)\n",
    "        xk = xk - alpha * DfxkT\n",
    "        \n",
    "    return 'Did not converge'\n",
    "\n",
    "# Try running it here!\n",
    "def f(t):\n",
    "    x, y = t\n",
    "    return 100*(y - x**2)**2 + (1 - x)**2\n",
    "\n",
    "exact_grad_descent(f, (-2,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3910e591",
   "metadata": {},
   "source": [
    "#### Does it converge? If not, explain why not. If it does, how many iterations does it take to get within 1e-5 of the true minimizer?\n",
    "\n",
    "It converged in 8230 iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
